{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fb3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import maha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a776fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>published_at</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>article_type</th>\n",
       "      <th>new_content</th>\n",
       "      <th>new_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633fdfd13ffae8229d05cb33</td>\n",
       "      <td>التربية: تحويل 42 مدرسة إلى نظام الفترتين واست...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>husna</td>\n",
       "      <td>2022-10-06 07:33:00</td>\n",
       "      <td>2022-10-07 08:14:09.717000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['أكدت أمين عام  وزارة  التربية والتعليم للشؤو...</td>\n",
       "      <td>['التربية والتعليم', 'وزارة التربية والتعليم']</td>\n",
       "      <td>News</td>\n",
       "      <td>أكدت أمين عام  وزارة  التربية والتعليم للشؤون ...</td>\n",
       "      <td>[التربية والتعليم,  وزارة التربية والتعليم]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>633fdfd13ffae8229d05cb34</td>\n",
       "      <td>تكريما للمعلمين زيادة منح أبناء المعلمين 550 م...</td>\n",
       "      <td>https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...</td>\n",
       "      <td>husna</td>\n",
       "      <td>2022-10-05 11:46:00</td>\n",
       "      <td>2022-10-07 08:14:09.788000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['احتفلت  وزارة التربية والتعليم  بيوم المعلم ...</td>\n",
       "      <td>['مكرمة أبناء المعلمين', 'وزارة التربية والتعل...</td>\n",
       "      <td>News</td>\n",
       "      <td>احتفلت  وزارة التربية والتعليم  بيوم المعلم بت...</td>\n",
       "      <td>[مكرمة أبناء المعلمين,  وزارة التربية والتعليم]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  633fdfd13ffae8229d05cb33   \n",
       "1  633fdfd13ffae8229d05cb34   \n",
       "\n",
       "                                               title  \\\n",
       "0  التربية: تحويل 42 مدرسة إلى نظام الفترتين واست...   \n",
       "1  تكريما للمعلمين زيادة منح أبناء المعلمين 550 م...   \n",
       "\n",
       "                                                 url publisher  \\\n",
       "0  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...     husna   \n",
       "1  https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...     husna   \n",
       "\n",
       "          published_at                  crawled_at  summary  \\\n",
       "0  2022-10-06 07:33:00  2022-10-07 08:14:09.717000      NaN   \n",
       "1  2022-10-05 11:46:00  2022-10-07 08:14:09.788000      NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  ['أكدت أمين عام  وزارة  التربية والتعليم للشؤو...   \n",
       "1  ['احتفلت  وزارة التربية والتعليم  بيوم المعلم ...   \n",
       "\n",
       "                                                tags article_type  \\\n",
       "0     ['التربية والتعليم', 'وزارة التربية والتعليم']         News   \n",
       "1  ['مكرمة أبناء المعلمين', 'وزارة التربية والتعل...         News   \n",
       "\n",
       "                                         new_content  \\\n",
       "0  أكدت أمين عام  وزارة  التربية والتعليم للشؤون ...   \n",
       "1  احتفلت  وزارة التربية والتعليم  بيوم المعلم بت...   \n",
       "\n",
       "                                          new_tags  \n",
       "0      [التربية والتعليم,  وزارة التربية والتعليم]  \n",
       "1  [مكرمة أبناء المعلمين,  وزارة التربية والتعليم]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "husna = pd.read_csv(\"husna.csv\")\n",
    "\n",
    "# Make Copy\n",
    "husna_copy = husna.copy()\n",
    "\n",
    "# Delete Null Contents\n",
    "husna_copy = husna_copy[husna_copy['content'] != '[]']\n",
    "\n",
    "# Clean the content by making it a str not a list\n",
    "husna_copy['new_content'] = husna['content'].apply(lambda x: x[2:-2])\n",
    "\n",
    "# Clean tags\n",
    "def clean_tags(tag):\n",
    "    lst = [i.replace(\"'\",\"\") for i in tag.split(',')]\n",
    "    lst[0] = lst[0][1:]\n",
    "    lst[-1] = lst[-1][:-1]\n",
    "    return lst\n",
    "\n",
    "husna_copy['new_tags'] = husna_copy['tags'].apply(clean_tags)\n",
    "#husna_copy['new_tags'] = husna_copy['new_tags'].apply()\n",
    "\n",
    "content = list(husna_copy['new_content'])\n",
    "husna_copy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd18dde",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Basel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Basel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stops Words ---------------------------------------\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('arabic'))\n",
    "stop_word_comp = {\"،\",\"%\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
    "\n",
    "# Union of the two sets\n",
    "full_stops = stops.union(stop_word_comp)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([w for w in words if w not in full_stops and len(w) >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214e1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Others ---------------------------------------\n",
    "\n",
    "from maha.cleaners.functions import remove\n",
    "\n",
    "def removing(text):\n",
    "    \n",
    "    text = remove(text,\n",
    "           english=True, # English words\n",
    "           numbers=True, \n",
    "           harakat=True, \n",
    "           tatweel=True, # Tatweel \"-\"\n",
    "           all_harakat=True,\n",
    "           punctuations=True,\n",
    "           arabic_punctuations=True,\n",
    "           english_punctuations=True,\n",
    "           emails=True, # \"@\"\n",
    "           emojis=True, \n",
    "           links=True, # \"www......\"\n",
    "           mentions=True) # \"#\" \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d99aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ---------------------------------------\n",
    "\n",
    "from maha.cleaners.functions import normalize\n",
    "from maha.cleaners.functions import normalize_lam_alef\n",
    "from maha.cleaners.functions import normalize_small_alef\n",
    "\n",
    "def normalizing(text):\n",
    "    text = normalize(text, all=True) # Make All the normalization proccesses\n",
    "    text = normalize_lam_alef(text) # ت ى --> تا\n",
    "#    text = normalize_small_alef(text) #  عمّ -->  عمى\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0500d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming ---------------------------------------\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "ArListem = ArabicLightStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    words = text.split()\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ArListem.light_stem(w)\n",
    "        cleaned.append(ArListem.get_root())\n",
    "        \n",
    "    return \" \".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full PreProcessing\n",
    "def processing(text):\n",
    "    \n",
    "    text = remove_stop_words(text)\n",
    "    text = removing(text)\n",
    "    text = normalizing(text)\n",
    "    text = stemming(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fdb8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'وكد مين وزر ربه علم لشو لمل ودر دكتر نجو قبل وسع دوم طلب درس وزر ربه نظم فرت جوء كتظظ زود عدد طلب نقل درس حكم نقص لبن درس وشر قبل حول درس نظم فرت يجر درس عوم علم بون درس حكم عمل نظم فرت درس بنت قبل ل حسن حوج وزر ربه بنن بنن درس جدد لسن ل قبل فقق رتج ضعع وزر ربه فيما علق برع وطن بنن درس نطق رحب حفظ فرق زول نظر نصف سلم وزر ربه وكد قبل وزر سلم درس مرر حوج درس جرء نوه وزر مدر لبن طلب جمع علم وكد سلم لبن وكد قبل وزر نجز مسح وطن عرف حوج درس وزع خلف حفظ نسب برع سوء برع وبن درس ورض قد طلب وزر برع نسق سبق مع حدد لمك رحل لبن حدد لحج من'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "processing(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5326e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_content = [processing(i) for i in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04c6439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1110, 531)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content[0]), len(clean_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb10826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  1354342\n",
      "After:  1017634\n",
      "Deleted:  336708\n"
     ]
    }
   ],
   "source": [
    "sum1, sum2 = 0, 0\n",
    "\n",
    "for sample in content:\n",
    "    sum1 += len(sample.split(\" \"))\n",
    "\n",
    "for sample in clean_content:\n",
    "    sum2 += len(sample.split(\" \"))\n",
    "    \n",
    "print(\"Before: \", sum1)\n",
    "print(\"After: \", sum2)\n",
    "print(\"Deleted: \", sum1 - sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f32f70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus: list[str], min_frequency: int = None):\n",
    "        self.min_frequency = min_frequency\n",
    "        self.vocab = self._create_vocab(corpus=corpus)\n",
    "        \n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        ...\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        ...\n",
    "    \n",
    "    def tokenize(self, documents: list[str]) -> list[list[int]]:\n",
    "        return [self._tokenize_document(document) for document in documents]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab\n",
    "\n",
    "class WordLevelTokenizer(Tokenizer):\n",
    "    def __init__(self, corpus: list[str], min_frequency: int = 0):\n",
    "        super().__init__(corpus=corpus, min_frequency=min_frequency)\n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        tokens_counter = Counter([token for sample in corpus for token in sample.split(\" \")])\n",
    "        tokens = [token for token, count in tokens_counter.items() if count >= self.min_frequency]\n",
    "        vocab = {token: index for index, token in enumerate(tokens, start=2)} \n",
    "        vocab[\"[PAD]\"] = 0\n",
    "        vocab[\"[OOV]\"] = 1\n",
    "        return vocab\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        return [self.vocab.get(token, -1) for token in document.split(\" \")]\n",
    "\n",
    "word_level_tokenizer = WordLevelTokenizer(corpus=clean_content, min_frequency=5)\n",
    "vocab = word_level_tokenizer.vocab\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20896f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 3967)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(vocabulary=vocab)\n",
    "tf_idf = tf.fit_transform(clean_content)\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac02a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \n",
    "    query = processing(query)\n",
    "    query = tf.transform([query])\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ff45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def output(query):\n",
    "    \n",
    "    query = process_query(query)\n",
    "    \n",
    "    cosine_similarities = linear_kernel(query, tf_idf).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "    \n",
    "    print(\"The Article Index are: \", related_docs_indices)\n",
    "    print(\"The Similarity (Score): \", cosine_similarities[related_docs_indices])\n",
    "    \n",
    "    print(\"Article Titles: \", husna_copy['title'].iloc[related_docs_indices])\n",
    "    print(\"Article URL's: \", husna_copy['url'].iloc[related_docs_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda830c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Article Index are:  [1297 2211 4382 1348   14]\n",
      "The Similarity (Score):  [0.35033909 0.34638262 0.31604884 0.30825621 0.29161237]\n",
      "Article Titles:  1297              62 وفاة و3145 اصابة بفيروس كورونا اليوم\n",
      "2336    307522 شخصاً حصلوا على الجرعة الأولى من لقاح ك...\n",
      "5282    350997 شخصاً حصلوا على الجرعة الاولى من لقاح ك...\n",
      "1348    تركيا :تطعيم 50 مليون شخص ضد كورونا حتى الخريف...\n",
      "14      البلبيسي: الإجراءات التخفيفية المتعلقة بكورونا...\n",
      "Name: title, dtype: object\n",
      "Article URL's:  1297    https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/62-%...\n",
      "2336    https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/3075...\n",
      "5282    https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/3509...\n",
      "1348    https://husna.fm/%D8%B9%D8%B1%D8%A8%D9%8A-%D9%...\n",
      "14      https://husna.fm/%D9%85%D8%AD%D9%84%D9%8A/%D8%...\n",
      "Name: url, dtype: object\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Search Here: \")\n",
    "\n",
    "output(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6025e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"TF_IDF_Matrix.txt\", 'wb')\n",
    "pickle.dump(tf_idf, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b4b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"cleaned_content.txt\", 'wb')\n",
    "pickle.dump(clean_content, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebec53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"vocab.txt\", 'wb')\n",
    "pickle.dump(vocab, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdcbad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "husna_copy.to_csv(\"husna_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff972c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
